{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f8aba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import pickle  # For saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0df7365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Defining the Tic Tac Toe game\n",
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((3, 3), dtype=int)  # 0: empty, 1: X, -1: O\n",
    "        self.current_player = 1  # Start with X (1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((3, 3), dtype=int)\n",
    "        self.current_player = 1\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        # Flatten the board to a tuple for hashing\n",
    "        return tuple(self.board.flatten())\n",
    "\n",
    "    def available_actions(self):\n",
    "        return [(i, j) for i in range(3) for j in range(3) if self.board[i, j] == 0]\n",
    "\n",
    "    def make_move(self, action):\n",
    "        i, j = action\n",
    "        if self.board[i, j] == 0:\n",
    "            self.board[i, j] = self.current_player\n",
    "            self.current_player = -self.current_player  # Switch player\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def check_winner(self):\n",
    "        # Check rows, columns, diagonals\n",
    "        for i in range(3):\n",
    "            if abs(np.sum(self.board[i, :])) == 3:\n",
    "                return self.board[i, 0]\n",
    "            if abs(np.sum(self.board[:, i])) == 3:\n",
    "                return self.board[0, i]\n",
    "        if abs(np.trace(self.board)) == 3 or abs(self.board[::-1].trace()) == 3:\n",
    "            return self.board[1, 1] if self.board[1, 1] != 0 else 0\n",
    "        return 0\n",
    "\n",
    "    def is_done(self):\n",
    "        winner = self.check_winner()\n",
    "        if winner != 0:\n",
    "            return True, winner\n",
    "        if len(self.available_actions()) == 0:\n",
    "            return True, 0  # Draw\n",
    "        return False, 0\n",
    "\n",
    "    def print_board(self):\n",
    "        symbols = {1: 'X', -1: 'O', 0: ' '}\n",
    "        for row in self.board:\n",
    "            print(' | '.join(symbols[cell] for cell in row))\n",
    "            print('-' * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef8fcd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) Building the reinforcement learning model\n",
    "# Using Q-learning with a tabular approach. Q-table indexed by state-action pairs.\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, player=1, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.player = player\n",
    "        self.q_table = defaultdict(lambda: defaultdict(float))\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "\n",
    "    def get_action(self, state, available_actions):\n",
    "        state_tuple = tuple(state.flatten())\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(available_actions)\n",
    "        q_values = self.q_table[state_tuple]\n",
    "        best_action = max(available_actions, key=lambda a: q_values[(a[0], a[1])])\n",
    "        return best_action\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        state_tuple = tuple(state.flatten())\n",
    "        next_state_tuple = tuple(next_state.flatten())\n",
    "        old_value = self.q_table[state_tuple][action]\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            available_next = TicTacToe().available_actions()  # Approximate, but in practice, pass it\n",
    "            # For simplicity, we assume opponent plays randomly or fixed, but here we max over possible\n",
    "            next_q = max(self.q_table[next_state_tuple].values()) if self.q_table[next_state_tuple] else 0\n",
    "            target = reward + self.gamma * next_q\n",
    "        new_value = old_value + self.alpha * (target - old_value)\n",
    "        self.q_table[state_tuple][action] = new_value\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(dict(self.q_table), f)\n",
    "\n",
    "    def load_model(self, filename):\n",
    "        try:\n",
    "            with open(filename, 'rb') as f:\n",
    "                self.q_table = defaultdict(lambda: defaultdict(float), pickle.load(f))\n",
    "        except FileNotFoundError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1c57384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d) Training the model\n",
    "def train_model(episodes=10000):\n",
    "    env = TicTacToe()\n",
    "    agent_x = QLearningAgent(player=1)\n",
    "    agent_o = QLearningAgent(player=-1)  # Opponent also learns, self-play\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if env.current_player == 1:\n",
    "                agent = agent_x\n",
    "                opp_agent = agent_o\n",
    "            else:\n",
    "                agent = agent_o\n",
    "                opp_agent = agent_x\n",
    "\n",
    "            available = env.available_actions()\n",
    "            action = agent.get_action(env.board, available)\n",
    "            old_state = env.board.copy()\n",
    "            env.make_move(action)\n",
    "            next_state = env.board.copy()\n",
    "\n",
    "            done, reward = env.is_done()\n",
    "            if done:\n",
    "                if reward == agent.player:\n",
    "                    reward = 1\n",
    "                elif reward == -agent.player:\n",
    "                    reward = -1\n",
    "                else:\n",
    "                    reward = 0\n",
    "            else:\n",
    "                # Small negative for continuing\n",
    "                reward = -0.01\n",
    "\n",
    "            agent.update(old_state, action, reward, next_state, done)\n",
    "\n",
    "            # Opponent's turn is handled in the loop\n",
    "\n",
    "        if episode % 1000 == 0:\n",
    "            print(f\"Episode {episode} completed.\")\n",
    "\n",
    "    agent_x.save_model('x_agent.pkl')\n",
    "    agent_o.save_model('o_agent.pkl')\n",
    "    return agent_x, agent_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af9e5bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e) Testing the model\n",
    "def test_model(agent_x, agent_o, num_games=10):\n",
    "    env = TicTacToe()\n",
    "    wins_x, wins_o, draws = 0, 0, 0\n",
    "\n",
    "    for game in range(num_games):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        print(f\"\\nGame {game + 1}:\")\n",
    "        while not done:\n",
    "            if env.current_player == 1:\n",
    "                agent = agent_x\n",
    "            else:\n",
    "                agent = agent_o\n",
    "\n",
    "            available = env.available_actions()\n",
    "            # Use greedy policy for testing (epsilon=0)\n",
    "            state_tuple = tuple(env.board.flatten())\n",
    "            if agent.q_table[state_tuple]:\n",
    "                action = max(available, key=lambda a: agent.q_table[state_tuple][a])\n",
    "            else:\n",
    "                action = random.choice(available)\n",
    "\n",
    "            env.make_move(action)\n",
    "            env.print_board()\n",
    "\n",
    "            done, winner = env.is_done()\n",
    "            if done:\n",
    "                if winner == 1:\n",
    "                    wins_x += 1\n",
    "                    print(\"X wins!\")\n",
    "                elif winner == -1:\n",
    "                    wins_o += 1\n",
    "                    print(\"O wins!\")\n",
    "                else:\n",
    "                    draws += 1\n",
    "                    print(\"Draw!\")\n",
    "\n",
    "    print(f\"\\nResults after {num_games} games:\")\n",
    "    print(f\"X wins: {wins_x}, O wins: {wins_o}, Draws: {draws}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2a962bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Episode 0 completed.\n",
      "Episode 1000 completed.\n",
      "Episode 2000 completed.\n",
      "Episode 3000 completed.\n",
      "Episode 4000 completed.\n",
      "\n",
      "Loading models for testing...\n",
      "Testing the model...\n",
      "\n",
      "Game 1:\n",
      "X |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "X |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "  |   | O\n",
      "-----\n",
      "X |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "  | X | O\n",
      "-----\n",
      "X |   |  \n",
      "-----\n",
      "  | O |  \n",
      "-----\n",
      "  | X | O\n",
      "-----\n",
      "X |   |  \n",
      "-----\n",
      "  | O |  \n",
      "-----\n",
      "X | X | O\n",
      "-----\n",
      "X |   |  \n",
      "-----\n",
      "  | O | O\n",
      "-----\n",
      "X | X | O\n",
      "-----\n",
      "X |   | X\n",
      "-----\n",
      "  | O | O\n",
      "-----\n",
      "X | X | O\n",
      "-----\n",
      "X |   | X\n",
      "-----\n",
      "O | O | O\n",
      "-----\n",
      "X | X | O\n",
      "-----\n",
      "O wins!\n",
      "\n",
      "Game 2:\n",
      "X |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "X |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "  |   | O\n",
      "-----\n",
      "X |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "  | X | O\n",
      "-----\n",
      "X |   |  \n",
      "-----\n",
      "  | O |  \n",
      "-----\n",
      "  | X | O\n",
      "-----\n",
      "X |   |  \n",
      "-----\n",
      "  | O |  \n",
      "-----\n",
      "X | X | O\n",
      "-----\n",
      "X |   |  \n",
      "-----\n",
      "  | O | O\n",
      "-----\n",
      "X | X | O\n",
      "-----\n",
      "X |   |  \n",
      "-----\n",
      "X | O | O\n",
      "-----\n",
      "X | X | O\n",
      "-----\n",
      "X wins!\n",
      "\n",
      "Game 3:\n",
      "X |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "X |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "  |   | O\n",
      "-----\n",
      "X |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "  | X | O\n",
      "-----\n",
      "X |   |  \n",
      "-----\n",
      "  | O |  \n",
      "-----\n",
      "  | X | O\n",
      "-----\n",
      "X |   |  \n",
      "-----\n",
      "  | O |  \n",
      "-----\n",
      "X | X | O\n",
      "-----\n",
      "X |   |  \n",
      "-----\n",
      "  | O | O\n",
      "-----\n",
      "X | X | O\n",
      "-----\n",
      "X |   | X\n",
      "-----\n",
      "  | O | O\n",
      "-----\n",
      "X | X | O\n",
      "-----\n",
      "X |   | X\n",
      "-----\n",
      "O | O | O\n",
      "-----\n",
      "X | X | O\n",
      "-----\n",
      "O wins!\n",
      "\n",
      "Game 4:\n",
      "X |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "X |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "  |   | O\n",
      "-----\n",
      "X |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "  | X | O\n",
      "-----\n",
      "X |   |  \n",
      "-----\n",
      "  | O |  \n",
      "-----\n",
      "  | X | O\n",
      "-----\n",
      "X |   |  \n",
      "-----\n",
      "  | O |  \n",
      "-----\n",
      "X | X | O\n",
      "-----\n",
      "X |   |  \n",
      "-----\n",
      "  | O | O\n",
      "-----\n",
      "X | X | O\n",
      "-----\n",
      "X | X |  \n",
      "-----\n",
      "  | O | O\n",
      "-----\n",
      "X | X | O\n",
      "-----\n",
      "X | X | O\n",
      "-----\n",
      "  | O | O\n",
      "-----\n",
      "X | X | O\n",
      "-----\n",
      "O wins!\n",
      "\n",
      "Game 5:\n",
      "X |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "X |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "  |   | O\n",
      "-----\n",
      "X |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "  | X | O\n",
      "-----\n",
      "X |   |  \n",
      "-----\n",
      "  | O |  \n",
      "-----\n",
      "  | X | O\n",
      "-----\n",
      "X |   |  \n",
      "-----\n",
      "  | O |  \n",
      "-----\n",
      "X | X | O\n",
      "-----\n",
      "X |   |  \n",
      "-----\n",
      "  | O | O\n",
      "-----\n",
      "X | X | O\n",
      "-----\n",
      "X | X |  \n",
      "-----\n",
      "  | O | O\n",
      "-----\n",
      "X | X | O\n",
      "-----\n",
      "X | X | O\n",
      "-----\n",
      "  | O | O\n",
      "-----\n",
      "X | X | O\n",
      "-----\n",
      "O wins!\n",
      "\n",
      "Results after 5 games:\n",
      "X wins: 1, O wins: 4, Draws: 0\n"
     ]
    }
   ],
   "source": [
    "# Run training and testing\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Training the model...\")\n",
    "    agent_x, agent_o = train_model(5000)  # Reduced episodes for demo\n",
    "\n",
    "    print(\"\\nLoading models for testing...\")\n",
    "    # In practice, load if saved\n",
    "    # agent_x.load_model('x_agent.pkl')\n",
    "    # agent_o.load_model('o_agent.pkl')\n",
    "\n",
    "    print(\"Testing the model...\")\n",
    "    test_model(agent_x, agent_o, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "771008a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Episode 0 completed.\n",
      "Episode 1000 completed.\n",
      "Episode 2000 completed.\n",
      "Episode 3000 completed.\n",
      "Episode 4000 completed.\n",
      "\n",
      "Testing the model...\n",
      "\n",
      "### Game 1:\n",
      "\n",
      "**Move 1:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "\n",
      "\n",
      "**Move 2:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "|   | O |   |\n",
      "\n",
      "\n",
      "**Move 3:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   | X |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "|   | O |   |\n",
      "\n",
      "\n",
      "**Move 4:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   | X |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| O | O |   |\n",
      "\n",
      "\n",
      "**Move 5:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X | X | X |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| O | O |   |\n",
      "\n",
      "**X wins!**\n",
      "\n",
      "### Game 2:\n",
      "\n",
      "**Move 1:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "\n",
      "\n",
      "**Move 2:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "|   | O |   |\n",
      "\n",
      "\n",
      "**Move 3:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   | X |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "|   | O |   |\n",
      "\n",
      "\n",
      "**Move 4:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   | X |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| O | O |   |\n",
      "\n",
      "\n",
      "**Move 5:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X | X | X |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| O | O |   |\n",
      "\n",
      "**X wins!**\n",
      "\n",
      "### Game 3:\n",
      "\n",
      "**Move 1:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "\n",
      "\n",
      "**Move 2:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "|   | O |   |\n",
      "\n",
      "\n",
      "**Move 3:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   | X |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "|   | O |   |\n",
      "\n",
      "\n",
      "**Move 4:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   | X |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| O | O |   |\n",
      "\n",
      "\n",
      "**Move 5:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X | X | X |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| O | O |   |\n",
      "\n",
      "**X wins!**\n",
      "\n",
      "### Game 4:\n",
      "\n",
      "**Move 1:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "\n",
      "\n",
      "**Move 2:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "|   | O |   |\n",
      "\n",
      "\n",
      "**Move 3:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   | X |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "|   | O |   |\n",
      "\n",
      "\n",
      "**Move 4:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   | X |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| O | O |   |\n",
      "\n",
      "\n",
      "**Move 5:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X | X | X |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| O | O |   |\n",
      "\n",
      "**X wins!**\n",
      "\n",
      "### Game 5:\n",
      "\n",
      "**Move 1:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "\n",
      "\n",
      "**Move 2:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "|   | O |   |\n",
      "\n",
      "\n",
      "**Move 3:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   | X |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "|   | O |   |\n",
      "\n",
      "\n",
      "**Move 4:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   | X |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| O | O |   |\n",
      "\n",
      "\n",
      "**Move 5:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X | X | X |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| O | O |   |\n",
      "\n",
      "**X wins!**\n",
      "\n",
      "### Final Results after 5 games:\n",
      "**X wins: 5**, **O wins: 0**, **Draws: 0**\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import pickle  # For saving the model\n",
    "from IPython.display import Markdown, display  # For better formatting in Jupyter, but for console, we'll use print\n",
    "\n",
    "# a) Setting up the environment\n",
    "# We use numpy for array operations, random for exploration, and defaultdict for Q-table.\n",
    "# No external libraries beyond basics are needed.\n",
    "\n",
    "# b) Defining the Tic Tac Toe game\n",
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((3, 3), dtype=int)  # 0: empty, 1: X, -1: O\n",
    "        self.current_player = 1  # Start with X (1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((3, 3), dtype=int)\n",
    "        self.current_player = 1\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        # Flatten the board to a tuple for hashing\n",
    "        return tuple(self.board.flatten())\n",
    "\n",
    "    def available_actions(self):\n",
    "        return [(i, j) for i in range(3) for j in range(3) if self.board[i, j] == 0]\n",
    "\n",
    "    def make_move(self, action):\n",
    "        i, j = action\n",
    "        if self.board[i, j] == 0:\n",
    "            self.board[i, j] = self.current_player\n",
    "            self.current_player = -self.current_player  # Switch player\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def check_winner(self):\n",
    "        # Check rows, columns, diagonals\n",
    "        for i in range(3):\n",
    "            if abs(np.sum(self.board[i, :])) == 3:\n",
    "                return self.board[i, 0]\n",
    "            if abs(np.sum(self.board[:, i])) == 3:\n",
    "                return self.board[0, i]\n",
    "        if abs(np.trace(self.board)) == 3 or abs(self.board[::-1].trace()) == 3:\n",
    "            return self.board[1, 1] if self.board[1, 1] != 0 else 0\n",
    "        return 0\n",
    "\n",
    "    def is_done(self):\n",
    "        winner = self.check_winner()\n",
    "        if winner != 0:\n",
    "            return True, winner\n",
    "        if len(self.available_actions()) == 0:\n",
    "            return True, 0  # Draw\n",
    "        return False, 0\n",
    "\n",
    "    def print_board_markdown(self):\n",
    "        symbols = {1: 'X', -1: 'O', 0: ' '}\n",
    "        board_str = \"|   |   |   |\\n\"\n",
    "        board_str += \"|---|---|---|\\n\"\n",
    "        for i in range(3):\n",
    "            row = \"| \" + \" | \".join(symbols[self.board[i, j]] for j in range(3)) + \" |\\n\"\n",
    "            board_str += row\n",
    "            if i < 2:\n",
    "                board_str += \"|---|---|---|\\n\"\n",
    "        print(board_str)\n",
    "\n",
    "# c) Building the reinforcement learning model\n",
    "# Using Q-learning with a tabular approach. Q-table indexed by state-action pairs.\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, player=1, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.player = player\n",
    "        self.q_table = defaultdict(lambda: defaultdict(float))\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "\n",
    "    def get_action(self, state, available_actions):\n",
    "        state_tuple = tuple(state.flatten())\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(available_actions)\n",
    "        q_values = self.q_table[state_tuple]\n",
    "        best_action = max(available_actions, key=lambda a: q_values[(a[0], a[1])])\n",
    "        return best_action\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        state_tuple = tuple(state.flatten())\n",
    "        next_state_tuple = tuple(next_state.flatten())\n",
    "        old_value = self.q_table[state_tuple][action]\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            next_q = max(self.q_table[next_state_tuple].values()) if self.q_table[next_state_tuple] else 0\n",
    "            target = reward + self.gamma * next_q\n",
    "        new_value = old_value + self.alpha * (target - old_value)\n",
    "        self.q_table[state_tuple][action] = new_value\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(dict(self.q_table), f)\n",
    "\n",
    "    def load_model(self, filename):\n",
    "        try:\n",
    "            with open(filename, 'rb') as f:\n",
    "                self.q_table = defaultdict(lambda: defaultdict(float), pickle.load(f))\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "# d) Training the model\n",
    "def train_model(episodes=10000):\n",
    "    env = TicTacToe()\n",
    "    agent_x = QLearningAgent(player=1)\n",
    "    agent_o = QLearningAgent(player=-1)  # Opponent also learns, self-play\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if env.current_player == 1:\n",
    "                agent = agent_x\n",
    "                opp_agent = agent_o\n",
    "            else:\n",
    "                agent = agent_o\n",
    "                opp_agent = agent_x\n",
    "\n",
    "            available = env.available_actions()\n",
    "            action = agent.get_action(env.board, available)\n",
    "            old_state = env.board.copy()\n",
    "            env.make_move(action)\n",
    "            next_state = env.board.copy()\n",
    "\n",
    "            done, reward = env.is_done()\n",
    "            if done:\n",
    "                if reward == agent.player:\n",
    "                    reward = 1\n",
    "                elif reward == -agent.player:\n",
    "                    reward = -1\n",
    "                else:\n",
    "                    reward = 0\n",
    "            else:\n",
    "                # Small negative for continuing\n",
    "                reward = -0.01\n",
    "\n",
    "            agent.update(old_state, action, reward, next_state, done)\n",
    "\n",
    "        if episode % 1000 == 0:\n",
    "            print(f\"Episode {episode} completed.\")\n",
    "\n",
    "    agent_x.save_model('x_agent.pkl')\n",
    "    agent_o.save_model('o_agent.pkl')\n",
    "    return agent_x, agent_o\n",
    "\n",
    "# e) Testing the model\n",
    "def test_model(agent_x, agent_o, num_games=5):\n",
    "    wins_x, wins_o, draws = 0, 0, 0\n",
    "\n",
    "    for game in range(num_games):\n",
    "        env = TicTacToe()\n",
    "        done = False\n",
    "        print(f\"\\n### Game {game + 1}:\")\n",
    "        move_count = 0\n",
    "        while not done:\n",
    "            if env.current_player == 1:\n",
    "                agent = agent_x\n",
    "            else:\n",
    "                agent = agent_o\n",
    "\n",
    "            available = env.available_actions()\n",
    "            # Use greedy policy for testing (epsilon=0)\n",
    "            state_tuple = tuple(env.board.flatten())\n",
    "            if agent.q_table[state_tuple]:\n",
    "                action = max(available, key=lambda a: agent.q_table[state_tuple][a])\n",
    "            else:\n",
    "                action = random.choice(available)\n",
    "\n",
    "            env.make_move(action)\n",
    "            move_count += 1\n",
    "            print(f\"\\n**Move {move_count}:**\")\n",
    "            env.print_board_markdown()\n",
    "\n",
    "            done, winner = env.is_done()\n",
    "            if done:\n",
    "                if winner == 1:\n",
    "                    wins_x += 1\n",
    "                    print(\"**X wins!**\")\n",
    "                elif winner == -1:\n",
    "                    wins_o += 1\n",
    "                    print(\"**O wins!**\")\n",
    "                else:\n",
    "                    draws += 1\n",
    "                    print(\"**Draw!**\")\n",
    "\n",
    "    print(f\"\\n### Final Results after {num_games} games:\")\n",
    "    print(f\"**X wins: {wins_x}**, **O wins: {wins_o}**, **Draws: {draws}**\")\n",
    "\n",
    "# Run training and testing\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Training the model...\")\n",
    "    agent_x, agent_o = train_model(5000)  # Reduced episodes for demo\n",
    "\n",
    "    print(\"\\nTesting the model...\")\n",
    "    test_model(agent_x, agent_o, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56aa2248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model with epsilon decay for higher accuracy...\n",
      "Episode 0 completed. Epsilon X: 0.995, O: 0.995\n",
      "Episode 5000 completed. Epsilon X: 0.010, O: 0.010\n",
      "Episode 10000 completed. Epsilon X: 0.010, O: 0.010\n",
      "Episode 15000 completed. Epsilon X: 0.010, O: 0.010\n",
      "\n",
      "Testing the trained X agent against Random O...\n",
      "\n",
      "### Game 1 vs Random O:\n",
      "\n",
      "**After Move 1:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "\n",
      "\n",
      "**After Move 3:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "|   | O | X |\n",
      "\n",
      "\n",
      "**After Move 5:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "|   | O | X |\n",
      "|---|---|---|\n",
      "|   | O | X |\n",
      "\n",
      "\n",
      "**After Move 6:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X | O |   |\n",
      "|---|---|---|\n",
      "|   | O | X |\n",
      "|---|---|---|\n",
      "|   | O | X |\n",
      "\n",
      "**O wins!**\n",
      "\n",
      "### Game 2 vs Random O:\n",
      "\n",
      "**After Move 1:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "\n",
      "\n",
      "**After Move 3:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "| O | X |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "\n",
      "\n",
      "**After Move 5:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "| O | X |   |\n",
      "|---|---|---|\n",
      "|   | O | X |\n",
      "\n",
      "**X wins!**\n",
      "\n",
      "### Game 3 vs Random O:\n",
      "\n",
      "**After Move 1:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "\n",
      "\n",
      "**After Move 3:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X | O | X |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "\n",
      "\n",
      "**After Move 5:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X | O | X |\n",
      "|---|---|---|\n",
      "| O | X |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "\n",
      "\n",
      "**After Move 6:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X | O | X |\n",
      "|---|---|---|\n",
      "| O | X | O |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "\n",
      "\n",
      "**After Move 7:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X | O | X |\n",
      "|---|---|---|\n",
      "| O | X | O |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "\n",
      "**X wins!**\n",
      "\n",
      "### Game 4 vs Random O:\n",
      "\n",
      "**After Move 1:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "\n",
      "\n",
      "**After Move 3:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "|   | O |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "\n",
      "\n",
      "**After Move 5:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "| X | O |   |\n",
      "|---|---|---|\n",
      "| X | O |   |\n",
      "\n",
      "**X wins!**\n",
      "\n",
      "### Game 5 vs Random O:\n",
      "\n",
      "**After Move 1:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "\n",
      "\n",
      "**After Move 3:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "|   | O |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "\n",
      "\n",
      "**After Move 5:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "| O | O |   |\n",
      "|---|---|---|\n",
      "| X | X |   |\n",
      "\n",
      "\n",
      "**After Move 6:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X | O |   |\n",
      "|---|---|---|\n",
      "| O | O |   |\n",
      "|---|---|---|\n",
      "| X | X |   |\n",
      "\n",
      "\n",
      "**After Move 7:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X | O |   |\n",
      "|---|---|---|\n",
      "| O | O |   |\n",
      "|---|---|---|\n",
      "| X | X | X |\n",
      "\n",
      "**X wins!**\n",
      "\n",
      "### Game 6 vs Random O:\n",
      "\n",
      "**After Move 1:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "\n",
      "\n",
      "**After Move 3:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "| X |   | O |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "\n",
      "\n",
      "**After Move 5:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "| X |   | O |\n",
      "|---|---|---|\n",
      "| X |   | O |\n",
      "\n",
      "**X wins!**\n",
      "\n",
      "### Game 7 vs Random O:\n",
      "\n",
      "**After Move 1:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "\n",
      "\n",
      "**After Move 3:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X | X | O |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "\n",
      "\n",
      "**After Move 5:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X | X | O |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| O |   | X |\n",
      "\n",
      "\n",
      "**After Move 6:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X | X | O |\n",
      "|---|---|---|\n",
      "|   |   | O |\n",
      "|---|---|---|\n",
      "| O |   | X |\n",
      "\n",
      "\n",
      "**After Move 7:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X | X | O |\n",
      "|---|---|---|\n",
      "|   | X | O |\n",
      "|---|---|---|\n",
      "| O |   | X |\n",
      "\n",
      "**X wins!**\n",
      "\n",
      "### Game 8 vs Random O:\n",
      "\n",
      "**After Move 1:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "\n",
      "\n",
      "**After Move 3:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| O |   | X |\n",
      "\n",
      "\n",
      "**After Move 5:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   | O |\n",
      "|---|---|---|\n",
      "|   | X |   |\n",
      "|---|---|---|\n",
      "| O |   | X |\n",
      "\n",
      "**X wins!**\n",
      "\n",
      "### Game 9 vs Random O:\n",
      "\n",
      "**After Move 1:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "\n",
      "\n",
      "**After Move 3:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "|   | O |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "\n",
      "\n",
      "**After Move 5:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "| O | O |   |\n",
      "|---|---|---|\n",
      "| X | X |   |\n",
      "\n",
      "\n",
      "**After Move 6:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "| O | O |   |\n",
      "|---|---|---|\n",
      "| X | X | O |\n",
      "\n",
      "\n",
      "**After Move 7:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X | X |   |\n",
      "|---|---|---|\n",
      "| O | O |   |\n",
      "|---|---|---|\n",
      "| X | X | O |\n",
      "\n",
      "\n",
      "**After Move 8:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X | X | O |\n",
      "|---|---|---|\n",
      "| O | O |   |\n",
      "|---|---|---|\n",
      "| X | X | O |\n",
      "\n",
      "\n",
      "**After Move 9:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X | X | O |\n",
      "|---|---|---|\n",
      "| O | O | X |\n",
      "|---|---|---|\n",
      "| X | X | O |\n",
      "\n",
      "**Draw!**\n",
      "\n",
      "### Game 10 vs Random O:\n",
      "\n",
      "**After Move 1:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "\n",
      "\n",
      "**After Move 3:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "|   | X |   |\n",
      "|---|---|---|\n",
      "|   |   | O |\n",
      "\n",
      "\n",
      "**After Move 5:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "|   | X | X |\n",
      "|---|---|---|\n",
      "| O |   | O |\n",
      "\n",
      "\n",
      "**After Move 6:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n",
      "| O | X | X |\n",
      "|---|---|---|\n",
      "| O |   | O |\n",
      "\n",
      "\n",
      "**After Move 7:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   | X |\n",
      "|---|---|---|\n",
      "| O | X | X |\n",
      "|---|---|---|\n",
      "| O |   | O |\n",
      "\n",
      "\n",
      "**After Move 8:**\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   | X |\n",
      "|---|---|---|\n",
      "| O | X | X |\n",
      "|---|---|---|\n",
      "| O | O | O |\n",
      "\n",
      "**O wins!**\n",
      "\n",
      "### Final Results after 10 games vs Random O:\n",
      "**X wins: 7**, **O wins: 2**, **Draws: 1**\n",
      "**X Win Rate (Accuracy): 70.0%**\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import pickle  # For saving the model\n",
    "\n",
    "# a) Setting up the environment\n",
    "# We use numpy for array operations, random for exploration, and defaultdict for Q-table.\n",
    "# No external libraries beyond basics are needed.\n",
    "\n",
    "# b) Defining the Tic Tac Toe game\n",
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((3, 3), dtype=int)  # 0: empty, 1: X, -1: O\n",
    "        self.current_player = 1  # Start with X (1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((3, 3), dtype=int)\n",
    "        self.current_player = 1\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        # Flatten the board to a tuple for hashing\n",
    "        return tuple(self.board.flatten())\n",
    "\n",
    "    def available_actions(self):\n",
    "        return [(i, j) for i in range(3) for j in range(3) if self.board[i, j] == 0]\n",
    "\n",
    "    def make_move(self, action):\n",
    "        i, j = action\n",
    "        if self.board[i, j] == 0:\n",
    "            self.board[i, j] = self.current_player\n",
    "            self.current_player = -self.current_player  # Switch player\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def check_winner(self):\n",
    "        # Check rows, columns, diagonals\n",
    "        for i in range(3):\n",
    "            if abs(np.sum(self.board[i, :])) == 3:\n",
    "                return self.board[i, 0]\n",
    "            if abs(np.sum(self.board[:, i])) == 3:\n",
    "                return self.board[0, i]\n",
    "        if abs(np.trace(self.board)) == 3 or abs(self.board[::-1].trace()) == 3:\n",
    "            return self.board[1, 1] if self.board[1, 1] != 0 else 0\n",
    "        return 0\n",
    "\n",
    "    def is_done(self):\n",
    "        winner = self.check_winner()\n",
    "        if winner != 0:\n",
    "            return True, winner\n",
    "        if len(self.available_actions()) == 0:\n",
    "            return True, 0  # Draw\n",
    "        return False, 0\n",
    "\n",
    "    def print_board_markdown(self):\n",
    "        symbols = {1: 'X', -1: 'O', 0: ' '}\n",
    "        board_str = \"|   |   |   |\\n\"\n",
    "        board_str += \"|---|---|---|\\n\"\n",
    "        for i in range(3):\n",
    "            row = \"| \" + \" | \".join(symbols[self.board[i, j]] for j in range(3)) + \" |\\n\"\n",
    "            board_str += row\n",
    "            if i < 2:\n",
    "                board_str += \"|---|---|---|\\n\"\n",
    "        print(board_str)\n",
    "\n",
    "# Random Agent for testing against random opponent\n",
    "class RandomAgent:\n",
    "    def get_action(self, state, available_actions):\n",
    "        return random.choice(available_actions)\n",
    "\n",
    "# c) Building the reinforcement learning model\n",
    "# Using Q-learning with epsilon decay for better convergence and accuracy\n",
    "class QLearningAgent:\n",
    "    def __init__(self, player=1, alpha=0.1, gamma=0.95, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        self.player = player\n",
    "        self.q_table = defaultdict(lambda: defaultdict(float))\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "    def get_action(self, state, available_actions):\n",
    "        state_tuple = tuple(state.flatten())\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(available_actions)\n",
    "        q_values = self.q_table[state_tuple]\n",
    "        best_action = max(available_actions, key=lambda a: q_values[(a[0], a[1])])\n",
    "        return best_action\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        state_tuple = tuple(state.flatten())\n",
    "        next_state_tuple = tuple(next_state.flatten())\n",
    "        old_value = self.q_table[state_tuple][action]\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            next_q = max(self.q_table[next_state_tuple].values()) if self.q_table[next_state_tuple] else 0\n",
    "            target = reward + self.gamma * next_q\n",
    "        new_value = old_value + self.alpha * (target - old_value)\n",
    "        self.q_table[state_tuple][action] = new_value\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(dict(self.q_table), f)\n",
    "\n",
    "    def load_model(self, filename):\n",
    "        try:\n",
    "            with open(filename, 'rb') as f:\n",
    "                self.q_table = defaultdict(lambda: defaultdict(float), pickle.load(f))\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    def set_greedy(self):\n",
    "        self.epsilon = 0  # For testing, no exploration\n",
    "\n",
    "# d) Training the model with self-play and epsilon decay\n",
    "def train_model(episodes=20000):\n",
    "    env = TicTacToe()\n",
    "    agent_x = QLearningAgent(player=1, epsilon=1.0)\n",
    "    agent_o = QLearningAgent(player=-1, epsilon=1.0)  # Self-play for learning\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if env.current_player == 1:\n",
    "                agent = agent_x\n",
    "            else:\n",
    "                agent = agent_o\n",
    "\n",
    "            available = env.available_actions()\n",
    "            if not available:\n",
    "                break\n",
    "            action = agent.get_action(env.board, available)\n",
    "            old_state = env.board.copy()\n",
    "            env.make_move(action)\n",
    "            next_state = env.board.copy()\n",
    "\n",
    "            done, reward = env.is_done()\n",
    "            if done:\n",
    "                if reward == agent.player:\n",
    "                    reward = 1\n",
    "                elif reward == -agent.player:\n",
    "                    reward = -1\n",
    "                else:\n",
    "                    reward = 0\n",
    "            else:\n",
    "                reward = -0.01  # Small penalty for longer games\n",
    "\n",
    "            agent.update(old_state, action, reward, next_state, done)\n",
    "\n",
    "        agent_x.decay_epsilon()\n",
    "        agent_o.decay_epsilon()\n",
    "\n",
    "        if episode % 5000 == 0:\n",
    "            print(f\"Episode {episode} completed. Epsilon X: {agent_x.epsilon:.3f}, O: {agent_o.epsilon:.3f}\")\n",
    "\n",
    "    agent_x.save_model('x_agent.pkl')\n",
    "    agent_o.save_model('o_agent.pkl')\n",
    "    agent_x.set_greedy()\n",
    "    agent_o.set_greedy()\n",
    "    return agent_x\n",
    "\n",
    "# e) Testing the model against a random opponent to evaluate accuracy\n",
    "def test_model_vs_random(agent_x, num_games=10):\n",
    "    wins_x, wins_o, draws = 0, 0, 0\n",
    "    random_agent = RandomAgent()\n",
    "\n",
    "    for game in range(num_games):\n",
    "        env = TicTacToe()\n",
    "        random.seed(game)  # Seed for reproducibility and variety\n",
    "        np.random.seed(game)\n",
    "        done = False\n",
    "        print(f\"\\n### Game {game + 1} vs Random O:\")\n",
    "        move_count = 0\n",
    "        while not done:\n",
    "            if env.current_player == 1:\n",
    "                agent = agent_x\n",
    "            else:\n",
    "                agent = random_agent\n",
    "\n",
    "            available = env.available_actions()\n",
    "            if not available:\n",
    "                break\n",
    "            if isinstance(agent, QLearningAgent):\n",
    "                state_tuple = tuple(env.board.flatten())\n",
    "                if agent.q_table[state_tuple]:\n",
    "                    action = max(available, key=lambda a: agent.q_table[state_tuple][a])\n",
    "                else:\n",
    "                    action = random.choice(available)\n",
    "            else:\n",
    "                action = agent.get_action(env.board, available)\n",
    "\n",
    "            env.make_move(action)\n",
    "            move_count += 1\n",
    "            # Print board after odd moves (X's turns) or at end\n",
    "            if move_count % 2 == 1 or move_count >= 5:\n",
    "                print(f\"\\n**After Move {move_count}:**\")\n",
    "                env.print_board_markdown()\n",
    "\n",
    "            done, winner = env.is_done()\n",
    "            if done:\n",
    "                if winner == 1:\n",
    "                    wins_x += 1\n",
    "                    print(\"**X wins!**\")\n",
    "                elif winner == -1:\n",
    "                    wins_o += 1\n",
    "                    print(\"**O wins!**\")\n",
    "                else:\n",
    "                    draws += 1\n",
    "                    print(\"**Draw!**\")\n",
    "\n",
    "    print(f\"\\n### Final Results after {num_games} games vs Random O:\")\n",
    "    print(f\"**X wins: {wins_x}**, **O wins: {wins_o}**, **Draws: {draws}**\")\n",
    "    win_rate = (wins_x / num_games) * 100\n",
    "    print(f\"**X Win Rate (Accuracy): {win_rate:.1f}%**\")\n",
    "\n",
    "# Run training and testing\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Training the model with epsilon decay for higher accuracy...\")\n",
    "    agent_x = train_model(20000)  # Train longer for better performance\n",
    "\n",
    "    print(\"\\nTesting the trained X agent against Random O...\")\n",
    "    test_model_vs_random(agent_x, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bacf5a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
